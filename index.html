<!-- Very simple website layout by me. Feel free to copy / adapt. -->

<!DOCTYPE html>
<html>
    <head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Yinkai Dong</title>

    <!-- <link href="https://fonts.googleapis.com/css?family=Roboto:100,400,700" rel="stylesheet"> -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="screen">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="assets/style.css" media="screen">

    <!-- Font awesome 5 (for icons) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css ">

    <link rel="icon" type="image/png" href="assets/img/favicon_32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="assets/img/favicon_16x16.png" sizes="16x16" />

    </head>

    <body>

    <header>
        <nav class="navbar navbar-expand-sm navbar-dark justify-content-center" style="background-color:#8a2731;">
            <a class="navbar-brand px-2" href="#">Yinkai Dong</a>

            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="w-50">
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ml-auto">
                        <li class="px-1 nav-item active"><a class="nav-link" href="#">About</a></li>
                        <li class="px-1 nav-item"><a class="nav-link" href="https://yinkai-dong.github.io/cv/Yinkai_Dong_CV_2024.pdf">CV</a></li>
                        <!-- <li class="px-1 nav-item"><a class="nav-link" href="https://github.com/yinkai-dong"><i class="fab fa-github" style="font-size:28px;"></i></a></li>
                        <li class="px-1 nav-item"><a class="nav-link" href="https://www.linkedin.com/in/raymondzl"><i class="fab fa-linkedin" style="font-size:28px;"></i></a></li> -->
                    </ul>
                </div>
            </div>
        </nav>
    </header>

    <main class="container">
        <hr>

        <div class="row justify-content-center">
            <div class="col-md-3">
                <img class="img-fluid rounded mx-auto d-block mb-2" id="profile" src="Photos/profile1.jpg" alt="Me">
            </div>
		
            <div class="col-md-8 pl-3" id="description">
                <p>Hello! I'm Yinkai Dong, a robotics enthusiast currently seeking a PhD position to further my research in robotics systems, including design, control, manipulation, and machine learning. My interests span human-robot interaction, exoskeletons, robotic manipulation, and assistive technologies aimed at improving the lives of disadvantaged individuals.</p> 
                <p>Currently, I am working with Prof. <a href=“https://seas.harvard.edu/person/patrick-slade”>Patrick Slade</a> at the <a href="sladelab.seas.harvard.edu">Harvard Slade Lab</a>on optimizing hip exoskeletons to enhance mobility and reduce energy cost.</p> 
                <p>Previously, I conducted research at <a href="https://www.eng.yale.edu/grablab/">Yale's GRAB Lab</a> under Prof. <a href="https://seas.yale.edu/faculty-research/faculty-directory/aaron-m-dollar">Aaron Dollar</a>, developing underactuated robotic hands for dexterous manipulation. I also participated in the special student program at MIT, took robotics-related courses, and joined <a href="https://newmanlab.mit.edu/">The Eric P. and Evelyn E. Newman Laboratory for Biomechanics and Human Rehabilitation</a> under the guidance of Prof. <a href="https://meche.mit.edu/people/faculty/neville@mit.edu">Neville Hogan</a>, focusing on impedance control and machine learning applications in robotics.</p>
                <p>My work has led to several publications and granted patents in the field of robotic grasping and manipulation devices.</p> 
                <p>Feel free to check out my CV for more details on my academic record, research projects, and technical skills.</p>                
            </p>
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Human-in-the-loop Optimization for Hip Exoskeleton</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-12" id="description">
                <p>I am working with Prof. Patrick Slade at the <a href="https://sladelab.seas.harvard.edu/">Harvard Slade Lab</a> to develop a hip exoskeleton system to assist human movement and reduce energy costs during walking.</p>
                <!-- <p>The exoskeleton is built using carbon fiber, machined parts, servos, and soft fabrics, providing a lightweight yet robust structure.</p> -->
                <!-- <p>We utilize an STM32 microcontroller for real-time control and Bluetooth data transmission, allowing for seamless monitoring and adjustments.</p> -->
                <p>I have developed a Python interface for the Delsys EMG sensor to enable real-time data streaming and processing, essential for evaluating muscle activity and performance.</p>
                <p>By implementing human-in-the-loop optimization strategies with reinforcement learning, we aim to enhance assistive performance, reduce energy costs, and improve adaptability to various tasks.</p>
                <p>Our overarching goal is to create an exoskeleton system that not only provides effective assistance but also adapts to individual users, ultimately enhancing mobility and quality of life.</p>
            </div>

            <!-- <div class="col">
                <img class="img-fluid mx-auto d-block" src="assets/img/hip_exoskeleton_figure.png" alt="Figure" style="max-width:85%;">
            </div> -->
        </div>
    

        <hr>
        <div class="col">
            <h4>Model Q-II: An Underactuated Hand with Enhanced Grasping Modes and Primitives for Dexterous Manipulation</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>During my time as a Research Assistant at the <a href="https://www.eng.yale.edu/grablab/">Yale GRAB Lab</a> under the supervision of Prof. <a href="https://seas.yale.edu/faculty-research/faculty-directory/aaron-d-dollar">Aaron Dollar</a>, I designed and implemented the Model Q-II gripper. This advanced underactuated robotic hand is capable of multiple grasping primitives, including pinch, tripod, quadpod, and power grasps.</p>
                <p>The gripper incorporates passive mechanisms that enable seamless transitions between grasping modes without additional actuators, significantly enhancing dexterity and manipulation capabilities. Through experimental validation, we achieved a forward manipulation range of 60.18 mm and a lateral range of 26.96 mm, while reducing the minimum graspable object diameter by 46%.</p>
                <p>The Model Q-II demonstrated effectiveness in precision tasks, power-driven operations, and complex in-hand manipulations, showcasing its potential for versatile robotic applications. Our work has been submitted to <strong>ICRA 2025</strong>.</p>
                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <!-- <a class="btn btn-outline-primary" href="URL_to_code">Code</a> -->
                        <a class="btn btn-outline-primary" href="Projects/Model Q-II/figure 1.jpg">Paper</a>
                        <a class="btn btn-outline-primary" href="Projects/Model Q-II/figure 1.jpg">Video</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid mb-2" src="Projects/Model Q-II/figure 1.jpg" alt="Model Q-II Gripper">
            </div>
        </div>


        <hr>
        <div class="col">
            <h4>Convolutional Transformers for Inertial Navigation</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>For my junior independent work at Princeton, I implemented neural network architectures that improve on existing state-of-the-art architectures on the task of inertial navigation. This task uses measurements from inertial measurement units (IMUs), which contain an accelerometer and a gyroscope, to predict an objects position. IMUs are cheap, ubiquitous, energy-efficient, and used in a wide variety of applications.</p>
                <p>My models use a transformer encoder and incorporate convolutional layers to extract both global and local data relationships, and achieve better results than the previous best method.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/yinkai-dong/ronin">Code</a>
                        <a class="btn btn-outline-primary" href="assets/pdf/iw_report.pdf">Paper</a>
                        <a class="btn btn-outline-primary" href="assets/img/iw_poster.pdf">Poster</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid" src="assets/img/cvit.png" alt="Ground truth and predicted trajectories from various models.">
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>OSCAR: Occluding Spatials, Category, And Region under discussion</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>For the final project for COS484 (Natural Language Processing) at Princeton, we reproduced a question-answering model and the ablations from <a href="https://aclanthology.org/2021.emnlp-main.390.pdf">this paper</a>. We additionally performed several of our own ablations, and tested using different models for generating question embeddings.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/ind1010/Region-under-discussion-for-visual-dialog">Code</a>
                        <a class="btn btn-outline-primary" href="assets/pdf/COS484_Report.pdf">Paper</a>
                        <a class="btn btn-outline-primary" href="assets/img/484poster.pdf">Poster</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid" src="assets/img/qcs_rud.png" alt="The QCS+RuD model.">
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Pedestrian Detection and Interpretability</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>For the final project for COS429 (Computer Vision) at Princeton, we investigated whether CNNs trained for object detections are reliant on visual cues when detecting pedestrians. We trained a Faster R-CNN on the Caltech Pedestrian Dataset, evaluated the model on different categories of pedestrian images, and improved the model by up-weighting images in poor-performing categories during training.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/ind1010/pedestrian_detection_interpretability">Code</a>
                        <a class="btn btn-outline-primary" href="assets/pdf/pedestrian_detection.pdf">Paper</a>
                        <a class="btn btn-outline-primary" href="assets/img/429poster.png">Poster</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid" src="assets/img/caltech_pedestrian.png" alt="Pedestrians">
                
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Crowdsourcing Datasets for Optical Flow</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>During summer 2020, I joined the <a href="https://pvl.cs.princeton.edu/">Princeton Vision & Learning Lab</a> to work on a visual learning project on optical flow. I helped develop and optimize a system for collecting human-annotated images.</p>
                <p>These annotations are used to predict the ground truth optical flow for various scenes and videos.</p>
            </div>

            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets/img/frame_0001.png" alt="Frame 1">
                <img class="img-fluid" src="assets/img/frame_0008.png" alt="Frame 8">
                
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Ray Tracing</h4>
        </div>
        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>During a summer internship at Oregon State University I created a simple ray tracer using C++. I implemented and tested methods for improving image rendering speed and making rendered images more realistic.</p>
                <p>The top image is an output from an early version of the ray tracer that used simple methods such as antialiasing and reflection.</p>
                <p>The bottom image is an output that used more advanced techniques, such as Monte Carlo path tracing, to create more realistic lighting.</p>
                <p>I presented my work at the <a href="https://issuu.com/saturdayacademy/docs/2018symposium_r3">2018 ASE Symposium</a> (see p.25) at the University of Portland.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/yinkai-dong/ray-tracing">Code</a>
                        <a class="btn btn-outline-primary" href="assets/pdf/rt_poster.pdf">Poster</a>
                        <a class="btn btn-outline-primary" href="assets/pdf/physically_based_rendering.pptx">Presentation</a>
                    </div>
                </div>
            </div>

            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets/img/rt_spheres.png" alt="Spheres">
                <img class="img-fluid" src="assets/img/path_tracing.png" alt="Path tracing">
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Explainable Neural Networks</h4>
        </div>

        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>During summer of 2019 I worked on a project at Oregon State University on explainable neural networks. This project aimed to explain the decision-making of deep neural nets for image recognition in terms of human concepts.</p>
                <p>The program was trained to recognize and identify images of birds and then analyzed to see whether it focused on semantically meaningful concepts, such as "Eye" or "Crown" as in the image shown here.</p>
                <!-- <p>The paper for this project is available <a href="https://arxiv.org/abs/1812.07150">here</a>. -->
            </div>

            <div class="col-md-4">
                <img class="img-fluid" src="assets/img/xnn.png" alt="XNN">
            </div>
        </div>

        <hr>
        <div class="col">
            <h4>Dementia Diagnosis</h4>
        </div>

        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>Over the course of 2016 and early 2017 I helped develop a project that used deep convolutional neural nets to analyze MRI scans and attempt to diagnose various stages of dementia. My motivation for this project comes from my family's history of Alzheimer's.</p>
                <p>By developing a technique that allowed the 3D MRI scans to each be split into hundreds of 2D "slices" as seen in the image, I was able to substantially improve the accuracy of the program.</p>
                <p>This project was submitted to the 2017 Central Western Oregon Science Expo and the subsequent Intel Northwest Science Expo, where it won several awards.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="assets/pdf/isef_poster.pdf">Poster</a>
                    </div>
                </div>
            </div>
                
            <div class="col-md-4">
                <img class="img-fluid" src="assets/img/mri_scan_1.png" alt="2D MRI scan">
                <img class="img-fluid" src="assets/img/mri_scan_2.png" alt="2D MRI scan">
                <img class="img-fluid" src="assets/img/mri_scan_3.png" alt="2D MRI scan">
            </div>
        </div>
       
        <hr>
        <div class="col">
            <h4>Other Websites</h4>
        </div>

        <div class="row pl-3 pr-3 mb-3">
            <div class="col-md-8" id="description">
                <p>Aside from this website, I've developed several other websites and concepts for websites. One example is <a href="https://airinchina.github.io/">airinchina.github.io</a>, which I created as an informative supplement for a high school project on air pollution in China.</p>
                <p>A test website I created as a structure for my final project for Web Development (CS290) at OSU can be viewed <a href="assets/riddet">here</a>. The full version had a functional database, but the search bar on the test version still works!</p>
            </div>
                
            <div class="col">
                <a href="https://airinchina.github.io/"><img class="img-fluid" src="assets/img/airchina.png" alt="Air in China"></a>
            </div>
        </div>

        <div class="row pl-3 pr-3 mb-3">
            <div class="col-md-8" id="description">
                <p>The most recent and the largest-scale website I've worked on so far is TigerTools, which I worked on alongside two of my classmates and friends, Indu Panigrahi and Adam Rebei. The website connects a simple and intuitive interface with a variety of APIs, allowing Princeton students to quickly locate on-campus amenities, such as printers, scanners, water filling stations, cafés, and vending machines.</p>
                <p>The latest version of TigerTools is currently hosted on <a href="https://www.tigerapps.org/#direct">TigerApps</a>. It requires a Princeton account to access.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/PrincetonUSG/TigerTools">Code</a>
                    </div>
                </div>
            </div>
                
            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets/img/tigertools.png" alt="TigerTools landing page">
                <img class="img-fluid" src="assets/img/tigertools2.png" alt="TigerTools interface">
            </div>
        </div>
        

        <div class="row pl-3 pr-3">
            <div class="col-md-8" id="description">
                <p>Another website I've developed is named QTPod, which provides a web interface for users to listen to podcasts with advertisements automatically blocked. The advertisements are detected with speech recognition and removed.</p>

                <div class="col d-flex justify-content-center mb-3">
                    <div class="btn-group" role="group" aria-label="Project links">
                        <a class="btn btn-outline-primary" href="https://github.com/QTPod/qtpod-site">Code</a>
                    </div>
                </div>
            </div>
                
            <div class="col-md-4">
                <img class="img-fluid mb-2" src="assets/img/qtpod_popular.png" alt="QTPod popular page">
                <img class="img-fluid" src="assets/img/qtpod_single.png" alt="Podcast interface">
            </div>
        </div>

        <hr>
        <div class="row">
            <img class="img-fluid mx-auto d-block" src="assets/img/footer.png" alt="Footer" style="max-width:60%;">
        </div>
        <hr>

    </main>

    </body>

</html>
